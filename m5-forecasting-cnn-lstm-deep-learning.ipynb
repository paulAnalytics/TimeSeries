{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The Makridakis competitions (or M-competitions), organised by forecasting expert Spyros Makridakis, aim to provide a better understanding and advancement of forecasting methodology by comparing the performance of different methods in solving a well-defined, real-world problem. The first M-competition was held in 1982. According to forecasting researcher and practitioner Rob Hyndman the M-competitions “have had an enormous influence on the field of forecasting. They focused attention on what models produced good forecasts, rather than on the mathematical properties of those models”. This empirical approach is very similar to Kaggle’s trade-mark way of having the best machine learning algorithms engage in intense competition on diverse datasets. M5 is the first M-competition to be held on Kaggle.\n",
    "\n",
    "\n",
    "## Objective\n",
    "\n",
    "To predict sales data provided by the retail giant Walmart 4 weeks into the future (two 2-week windows). \n",
    "\n",
    "\n",
    "## Dataset\n",
    "\n",
    "The data: We are working with 42,840 hierarchical time series. The data were obtained in the 3 US states of California (CA), Texas (TX), and Wisconsin (WI). “Hierarchical” here means that data can be aggregated on different levels: item level, department level, product category level, and state level. The sales information reaches back from Jan 2011 to June 2016. In addition to the sales numbers, we are also given corresponding data on prices, promotions, and holidays. The dataset is zero-inflated, that is entries with empty information are imputed with a zero value. Hence, most of the time series contain zero values.\n",
    "\n",
    "The data comprises 3049 individual products from 3 categories and 7 departments, sold in 10 stores in 3 states. The hierachical aggregation captures the combinations of these factors. For instance, we can create 1 time series for all sales, 3 time series for all sales per state, and so on. The largest category is sales of all individual 3049 products per 10 stores for 30490 time series.\n",
    "\n",
    "\n",
    "## Dataset files\n",
    "The training data comes in the shape of 3 separate files:\n",
    "\n",
    "`sales_train.csv`: this is our main training data. It has 1 column for each of the 1941 days from 2011-01-29 and 2016-05-22; not including the validation period of 28 days until 2016-06-19. It also includes the IDs for item, department, category, store, and state. The number of rows is 30490 for all combinations of 30490 items and 10 stores.\n",
    "\n",
    "`sell_prices.csv`: the store and item IDs together with the sales price of the item as a weekly average.\n",
    "\n",
    "`calendar.csv`: dates together with related features like day-of-the week, month, year, and an 3 binary flags for whether the stores in each state allowed purchases with SNAP food stamps at this date (1) or not (0).\n",
    "\n",
    "\n",
    "## Metrics\n",
    "\n",
    "The point forecast submission are being evaluated using the **Root Mean Squared Scaled Error (RMSSE)**, which is derived from the Mean Absolute Scaled Error (MASE) that was designed to be scale invariant and symmetric. In a similar way to the MASE, the RMSSE is scale invariant and symmetric, and measures the prediction error (i.e. forecast - truth) relative to a “naive forecast” that simply assumes that step i = step i-1. In contrast to the MASE, here both prediction error and naive error are scaled to account for the goal of estimating average values in the presence of many zeros.\n",
    "\n",
    "The metric is computed for each time series and then averaged accross all time series including weights. The weights are proportional to the sales volume of the item, in dollars, to give more importance to high selling products. Note, that the weights are based on the last 28 days of the training data, and that those dates will be adjusted for the ultimate evaluation data, as confirmed by the organisers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "* [ Load data and downcasting ](#downcast)\n",
    "\n",
    "* [ Preprocessing ](#preprocess)\n",
    "\n",
    "* [ CNN-LSTM Model](#cnnlstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"downcast\"></a>\n",
    "## Load data and downcasting\n",
    "\n",
    "Downcasting the dataframes helps to reduce the amount of storage used by them and also to expidite the operations performed on them.\n",
    "\n",
    "Numerical Columns: Depending on your environment, pandas automatically creates int32, int64, float32 or float64 columns for numeric ones. If you know the min or max value of a column, you can use a subtype which is less memory consuming. You can also use an unsigned subtype if there is no negative value. In this dataset, we convert features with a float64 data type into float32 data type, and convert features with int64, int32 to in16 data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os\n",
    "\n",
    "train_sales = pd.read_csv('Data/sales_train_evaluation.csv')\n",
    "calendar = pd.read_csv('Data/calendar.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#To reduce memory usage\n",
    "def downcast_dtypes(df):\n",
    "    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n",
    "    int_cols = [c for c in df if df[c].dtype in [\"int64\", \"int32\"]]\n",
    "    df[float_cols] = df[float_cols].astype(np.float32)\n",
    "    df[int_cols] = df[int_cols].astype(np.int16)\n",
    "    return df\n",
    "\n",
    "#Reduce memory usage and compare with the previous one to be sure\n",
    "train_sales = downcast_dtypes(train_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>d_1</th>\n",
       "      <th>d_2</th>\n",
       "      <th>d_3</th>\n",
       "      <th>d_4</th>\n",
       "      <th>...</th>\n",
       "      <th>d_1932</th>\n",
       "      <th>d_1933</th>\n",
       "      <th>d_1934</th>\n",
       "      <th>d_1935</th>\n",
       "      <th>d_1936</th>\n",
       "      <th>d_1937</th>\n",
       "      <th>d_1938</th>\n",
       "      <th>d_1939</th>\n",
       "      <th>d_1940</th>\n",
       "      <th>d_1941</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_002</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_003_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_003</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_004_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_004</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_005_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_005</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1947 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id        item_id    dept_id   cat_id store_id  \\\n",
       "0  HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n",
       "1  HOBBIES_1_002_CA_1_evaluation  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1   \n",
       "2  HOBBIES_1_003_CA_1_evaluation  HOBBIES_1_003  HOBBIES_1  HOBBIES     CA_1   \n",
       "3  HOBBIES_1_004_CA_1_evaluation  HOBBIES_1_004  HOBBIES_1  HOBBIES     CA_1   \n",
       "4  HOBBIES_1_005_CA_1_evaluation  HOBBIES_1_005  HOBBIES_1  HOBBIES     CA_1   \n",
       "\n",
       "  state_id  d_1  d_2  d_3  d_4  ...  d_1932  d_1933  d_1934  d_1935  d_1936  \\\n",
       "0       CA    0    0    0    0  ...       2       4       0       0       0   \n",
       "1       CA    0    0    0    0  ...       0       1       2       1       1   \n",
       "2       CA    0    0    0    0  ...       1       0       2       0       0   \n",
       "3       CA    0    0    0    0  ...       1       1       0       4       0   \n",
       "4       CA    0    0    0    0  ...       0       0       0       2       1   \n",
       "\n",
       "   d_1937  d_1938  d_1939  d_1940  d_1941  \n",
       "0       0       3       3       0       1  \n",
       "1       0       0       0       0       0  \n",
       "2       0       2       3       0       1  \n",
       "3       1       3       0       2       6  \n",
       "4       0       0       2       1       0  \n",
       "\n",
       "[5 rows x 1947 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sales.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"preprocess\"></a>\n",
    "## Preprocess dataset\n",
    "\n",
    "Based on the sell price dataset, there are multiple products which has zero prices from day 1 to day 350. This is due to the product not being listed in the store yet. Additionally, on the sales_train dataset, there are multiple products which has zero sales, This could be due to zero prices or that the sales is not recorded. \n",
    "\n",
    "Since our LSTM model predicts the future sales of multiple products instead of the sales of individual products, hence we need to set a cutoff data instead of trimming the dataset by the first non-zero prices. Hence we set this cutoff date on day 350 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>30480</th>\n",
       "      <th>30481</th>\n",
       "      <th>30482</th>\n",
       "      <th>30483</th>\n",
       "      <th>30484</th>\n",
       "      <th>30485</th>\n",
       "      <th>30486</th>\n",
       "      <th>30487</th>\n",
       "      <th>30488</th>\n",
       "      <th>30489</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>d_351</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d_352</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d_353</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d_354</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d_355</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30490 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1     2     3     4     5     6     7     8     9      ... 30480  \\\n",
       "d_351     0     0     0     2     0     0     0    24     3     2  ...     0   \n",
       "d_352     0     0     0     0     0     0     0     9     0     2  ...     0   \n",
       "d_353     0     0     0     4     2     0     0     2     1     1  ...     0   \n",
       "d_354     0     1     0     2     0     0     0     7     1     0  ...     0   \n",
       "d_355     0     0     0     1     2     0     0     0     0     0  ...     0   \n",
       "\n",
       "      30481 30482 30483 30484 30485 30486 30487 30488 30489  \n",
       "d_351     9     1     0    11     0     0     1     0     0  \n",
       "d_352     5     4     0     8     0     1     2     0     0  \n",
       "d_353    15     2     0     3     0     1     2     0     0  \n",
       "d_354     5     1     0     3     0     0     0     0     0  \n",
       "d_355     7     1     0     1     0     1     1     0     0  \n",
       "\n",
       "[5 rows x 30490 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess: remove id, item_id, dept_id, cat_id, store_id, state_id columns\n",
    "startDay = 350  # Remove the first 350 days in train sales data due to zero_inflated data\n",
    "train_sales = train_sales.T\n",
    "train_sales = train_sales[6 + startDay:]\n",
    "train_sales.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional features: event 1, event 2 and SNAP\n",
    "\n",
    "Besides using sales as a time series feature, festive or sports events can have a strong positive influence on the sales. For example, shoppers are more likely to purchase more snacks or food a day before Thanksgiving or Superbowl. Similarly, we expect more sales during the days with SNAP program. Hence, we inserted 5 additional features: event 1, event 2, SNAP_CA, SNAP_WI, SNAP_TX. The days of SNAP program is different for each store location and therefore we need 3 separate features for SNAP.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize a dataframe with zeros for 1969 days in the calendar\n",
    "\n",
    "daysBeforeEvent1 = pd.DataFrame(np.zeros((1969,1)))\n",
    "daysBeforeEvent2 = pd.DataFrame(np.zeros((1969,1)))\n",
    "\n",
    "snap_CA = pd.DataFrame(np.zeros((1969,1)))\n",
    "snap_TX = pd.DataFrame(np.zeros((1969,1)))\n",
    "snap_WI = pd.DataFrame(np.zeros((1969,1)))\n",
    "\n",
    "\n",
    "# Label 1 to one day before the event_name_1 \n",
    "# Label 1 to one day before the event_name_2\n",
    "# Sales are likely to increase one day before events like superbowl etc.\n",
    "\n",
    "# Label 1 to days on snap_CA\n",
    "# Label 1 to days on snap_TX\n",
    "# Label 1 to days on snap_WI\n",
    "\n",
    "for x,y in calendar.iterrows():\n",
    "    if((pd.isnull(calendar[\"event_name_1\"][x])) == False):\n",
    "           daysBeforeEvent1[0][x-1] = 1 \n",
    "            \n",
    "    if((pd.isnull(calendar[\"event_name_2\"][x])) == False):\n",
    "           daysBeforeEvent2[0][x-1] = 1    \n",
    "    \n",
    "    \n",
    "    if((pd.isnull(calendar[\"snap_CA\"][x])) == False):\n",
    "           snap_CA[0][x] = 1    \n",
    "        \n",
    "    if((pd.isnull(calendar[\"snap_TX\"][x])) == False):\n",
    "           snap_TX[0][x] = 1    \n",
    "        \n",
    "    if((pd.isnull(calendar[\"snap_WI\"][x])) == False):\n",
    "           snap_WI[0][x] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1964</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1965</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1966</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1968</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1969 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0\n",
       "0     0.0\n",
       "1     0.0\n",
       "2     0.0\n",
       "3     0.0\n",
       "4     0.0\n",
       "...   ...\n",
       "1964  0.0\n",
       "1965  0.0\n",
       "1966  0.0\n",
       "1967  1.0\n",
       "1968  0.0\n",
       "\n",
       "[1969 rows x 1 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daysBeforeEvent1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1964</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1965</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1966</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1968</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1969 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0\n",
       "0     0.0\n",
       "1     0.0\n",
       "2     0.0\n",
       "3     0.0\n",
       "4     0.0\n",
       "...   ...\n",
       "1964  0.0\n",
       "1965  0.0\n",
       "1966  0.0\n",
       "1967  1.0\n",
       "1968  0.0\n",
       "\n",
       "[1969 rows x 1 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daysBeforeEvent2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating training, validation, evaluation dataset\n",
    "\n",
    "**Training Dataset**\n",
    "- To train a timeseries dataset, we will use the training dataset, which comprises of features (sales, event 1, event 2, SNAP) from day 350 to 1912. \n",
    "\n",
    "- From this training dataset, we will predict the future sales of each product for 56 days in 2 week windows (predict day 1913 to 1940, and predict day 1941 to 1969).\n",
    "\n",
    "**Validation Dataset**\n",
    "- The dataset with the features (sales, event 1, event 2, SNAP) from day 1913 to 1940 is used to validate the predicted values from trained dataset. \n",
    "\n",
    "**Evaluation Dataset**\n",
    "- The dataset with the features (sales, event 1, event 2, SNAP) from day 1941 to 1969 is used to evaluate the predicted values from trained dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split dataset into evaluation (last 2 weeks), validation (first 2 weeks), training  \n",
    "# input for predicting validation period day 1941 to 1969\n",
    "\n",
    "daysBeforeEvent1_eval = daysBeforeEvent1[1941:]\n",
    "daysBeforeEvent2_eval = daysBeforeEvent2[1941:]\n",
    "\n",
    "snap_CA_eval = snap_CA[1941:]\n",
    "snap_TX_eval = snap_TX[1941:]\n",
    "snap_WI_eval = snap_WI[1941:]\n",
    "\n",
    "\n",
    "# input for predicting validation period day 1913 to 1941\n",
    "\n",
    "daysBeforeEvent1_valid = daysBeforeEvent1[1913:1941] \n",
    "daysBeforeEvent2_valid = daysBeforeEvent2[1913:1941]\n",
    "\n",
    "snap_CA_valid = snap_CA[1913:1941] \n",
    "snap_TX_valid = snap_TX[1913:1941]\n",
    "snap_WI_valid = snap_WI[1913:1941]\n",
    "\n",
    "# input for training as a feature\n",
    "# daysBeforeEvent1 = daysBeforeEvent1[startDay:1913] \n",
    "# daysBeforeEvent2 = daysBeforeEvent2[startDay:1913] \n",
    "daysBeforeEvent1 = daysBeforeEvent1[startDay:1941] \n",
    "daysBeforeEvent2 = daysBeforeEvent2[startDay:1941]\n",
    "\n",
    "snap_CA = snap_CA[startDay:1941] \n",
    "snap_TX = snap_TX[startDay:1941] \n",
    "snap_WI = snap_WI[startDay:1941] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Before concatanation with our main data \"dt\", indexes are made same and column name is changed to \"oneDayBeforeEvent\"\n",
    "daysBeforeEvent1.columns = [\"oneDayBeforeEvent1\"]\n",
    "daysBeforeEvent1.index = train_sales.index\n",
    "\n",
    "daysBeforeEvent2.columns = [\"oneDayBeforeEvent2\"]\n",
    "daysBeforeEvent2.index = train_sales.index\n",
    "\n",
    "\n",
    "snap_CA.columns = [\"snap_CA\"]\n",
    "snap_CA.index = train_sales.index\n",
    "\n",
    "snap_TX.columns = [\"snap_TX\"]\n",
    "snap_TX.index = train_sales.index\n",
    "\n",
    "snap_WI.columns = [\"snap_WI\"]\n",
    "snap_WI.index = train_sales.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>30485</th>\n",
       "      <th>30486</th>\n",
       "      <th>30487</th>\n",
       "      <th>30488</th>\n",
       "      <th>30489</th>\n",
       "      <th>oneDayBeforeEvent1</th>\n",
       "      <th>oneDayBeforeEvent2</th>\n",
       "      <th>snap_CA</th>\n",
       "      <th>snap_TX</th>\n",
       "      <th>snap_WI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>d_351</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d_352</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d_353</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d_354</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d_355</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30495 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0  1  2  3  4  5  6   7  8  9  ... 30485 30486 30487 30488 30489  \\\n",
       "d_351  0  0  0  2  0  0  0  24  3  2  ...     0     0     1     0     0   \n",
       "d_352  0  0  0  0  0  0  0   9  0  2  ...     0     1     2     0     0   \n",
       "d_353  0  0  0  4  2  0  0   2  1  1  ...     0     1     2     0     0   \n",
       "d_354  0  1  0  2  0  0  0   7  1  0  ...     0     0     0     0     0   \n",
       "d_355  0  0  0  1  2  0  0   0  0  0  ...     0     1     1     0     0   \n",
       "\n",
       "      oneDayBeforeEvent1 oneDayBeforeEvent2 snap_CA snap_TX snap_WI  \n",
       "d_351                0.0                0.0     1.0     1.0     1.0  \n",
       "d_352                1.0                0.0     1.0     1.0     1.0  \n",
       "d_353                0.0                0.0     1.0     1.0     1.0  \n",
       "d_354                0.0                0.0     1.0     1.0     1.0  \n",
       "d_355                0.0                0.0     1.0     1.0     1.0  \n",
       "\n",
       "[5 rows x 30495 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sales = pd.concat([train_sales, daysBeforeEvent1, daysBeforeEvent2,\n",
    "                        snap_CA, snap_TX, snap_WI], axis = 1, sort=False)\n",
    "train_sales.head()  # additional features (event1, event2, SNAP) are added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardizing features\n",
    "\n",
    "- It is also important to scale our features across the columns. Each columns represents the sales values of a particular day. This helps to ensure that the sales values are between 0 and 1, and this helps the gradient descent optimization algorithm in LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Feature Scaling: Scale features using min-max scaler in range 0-1\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "sc = MinMaxScaler(feature_range = (0, 1))\n",
    "train_sales.columns = train_sales.columns.astype(str)\n",
    "train_sales_scaled = sc.fit_transform(train_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "timesteps = 28  # use the last 28 days to predict the next day's sales\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(timesteps, 1941 - startDay):\n",
    "    X_train.append(train_sales_scaled[i-timesteps:i])\n",
    "    y_train.append(train_sales_scaled[i][0:30490])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1563, 28, 30495)\n",
      "(1563, 30490)\n"
     ]
    }
   ],
   "source": [
    "#Convert to np array to be able to feed the LSTM model\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1563, 28, 30495)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1563, 30490)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"cnnlstm\"></a>\n",
    "## CNN-LSTM Model\n",
    "\n",
    "The model used is CNN-LSTM, with regularization parameters such as batch normalization. There are 2 units of CNN (conv1d followed by max pooling layer). The conv1d layer helps to analyse and extract the patterns of the sales in 1 week window (just like extracting \"edges\" patterns in a typcial conv2d on an image). The max pooling layer then summarizes these patterns into broader, generalizable patterns, which  seeks to remove noises in the spikes or troughs in daily sales.\n",
    "\n",
    "The outputs from CNN is then fed into a LSTM model with 3 layers of bidirectional LSTM. We use an inverted number of LSTM units from 512 to 256 to 128. This has the property of reducing and summarizing the sales patterns for better prediction. The batch normalization helps to improve the robustness of the model by reducing the likelihood of overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4000)])\n",
    "  except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if gpus:\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4000)])\n",
    "  except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.14.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 28, 128)           27323648  \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1  (None, 14, 128)           0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 14, 64)            57408     \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPoolin  (None, 7, 64)             0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 7, 1024)           2363392   \n",
      " al)                                                             \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 7, 1024)           4096      \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirecti  (None, 7, 512)            2623488   \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 7, 512)            2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirecti  (None, 256)               656384    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 256)               1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense (Dense)               (None, 30490)             7835930   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40867418 (155.90 MB)\n",
      "Trainable params: 40863834 (155.88 MB)\n",
      "Non-trainable params: 3584 (14.00 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM,Dropout\n",
    "\n",
    "\n",
    "tf.random.set_seed(51)\n",
    "np.random.seed(51)\n",
    "\n",
    "n_timesteps = X_train.shape[1]\n",
    "n_products = X_train.shape[2]\n",
    "\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv1D(filters=128, kernel_size=7,\n",
    "                      strides=1, padding=\"causal\",\n",
    "                      activation=\"relu\",\n",
    "                      input_shape=(n_timesteps, n_products)),\n",
    "    tf.keras.layers.MaxPooling1D(),\n",
    "    tf.keras.layers.Conv1D(filters=64, kernel_size=7, \n",
    "                           strides=1, activation='relu', padding=\"causal\"),\n",
    "    tf.keras.layers.MaxPooling1D(),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512, return_sequences=True)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(30490)\n",
    "])\n",
    "\n",
    "\n",
    "opt_adam = tf.keras.optimizers.Adam(clipvalue=0.5)\n",
    "\n",
    "model.compile(loss='mse',\n",
    "              optimizer=opt_adam, \n",
    "              metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks\n",
    "\n",
    "Callbacks are inserted to monitor the performance of model after 100 epochs. If the performance of the model degrades, then the model is stopped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a Callback class that stops training when the val_loss degrades above 150 epochs\n",
    "# and also saves the model as checkpoints\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fitting the RNN to the Training set\n",
    "epochs = 150\n",
    "batch_size = 1\n",
    "model.fit(X_train, y_train, epochs = epochs, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_eval = train_sales[-timesteps:]\n",
    "inputs_eval = sc.transform(inputs_eval)\n",
    "\n",
    "inputs = train_sales[-timesteps*2:-timesteps]\n",
    "inputs = sc.transform(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the future sales for validation and evaluation periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "X_test.append(inputs[0:timesteps])\n",
    "X_test = np.array(X_test)\n",
    "predictions = []\n",
    "\n",
    "for j in range(timesteps,timesteps + 28):\n",
    "    predicted_stock_price = model.predict(X_test[0,j - timesteps:j].reshape(1, timesteps, 30495))\n",
    "    \n",
    "    testInput = np.column_stack((np.array(predicted_stock_price),\n",
    "                                 daysBeforeEvent1_valid.loc[1913 + j - timesteps],\n",
    "                                 daysBeforeEvent2_valid.loc[1913 + j - timesteps],\n",
    "                                 snap_CA_valid.loc[1913 + j - timesteps],\n",
    "                                snap_TX_valid.loc[1913 + j - timesteps],\n",
    "                                snap_WI_valid.loc[1913 + j - timesteps]))\n",
    "\n",
    "    X_test = np.append(X_test, testInput).reshape(1,j + 1,30495)\n",
    "    predicted_stock_price = sc.inverse_transform(testInput)[:,0:30490]\n",
    "    predictions.append(predicted_stock_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_eval = []\n",
    "X_eval.append(inputs_eval[0:timesteps])\n",
    "X_eval = np.array(X_eval)\n",
    "predictions_eval = []\n",
    "\n",
    "for j in range(timesteps,timesteps + 28):\n",
    "    predicted_stock_price = model2.predict(X_eval[0,j - timesteps:j].reshape(1, timesteps, 30495))\n",
    "    \n",
    "    testInput = np.column_stack((np.array(predicted_stock_price),\n",
    "                                 daysBeforeEvent1_eval.loc[1941 + j - timesteps],\n",
    "                                 daysBeforeEvent2_eval.loc[1941 + j - timesteps],\n",
    "                                 snap_CA_eval.loc[1941 + j - timesteps],\n",
    "                                snap_TX_eval.loc[1941 + j - timesteps],\n",
    "                                snap_WI_eval.loc[1941 + j - timesteps]))\n",
    "\n",
    "    X_eval = np.append(X_eval, testInput).reshape(1,j + 1,30495)\n",
    "    predicted_stock_price = sc.inverse_transform(testInput)[:,0:30490]\n",
    "    predictions_eval.append(predicted_stock_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post processing of future sales\n",
    "\n",
    "For sales with predicted negative values, they are converted into zeros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "submission = pd.DataFrame(data=np.array(predictions).reshape(28,30490))\n",
    "submission = submission.T\n",
    "\n",
    "submission_eval = pd.DataFrame(data=np.array(predictions_eval).reshape(28,30490))\n",
    "submission_eval = submission_eval.T\n",
    "\n",
    "submission = pd.concat((submission, submission_eval), ignore_index=True)\n",
    "\n",
    "sample_submission = pd.read_csv(\"sample_submission.csv\")\n",
    "    \n",
    "idColumn = sample_submission[[\"id\"]]\n",
    "    \n",
    "submission[[\"id\"]] = idColumn  \n",
    "\n",
    "cols = list(submission.columns)\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "submission = submission[cols]\n",
    "\n",
    "colsdeneme = [\"id\"] + [f\"F{i}\" for i in range (1,29)]\n",
    "\n",
    "submission.columns = colsdeneme\n",
    "\n",
    "currentDateTime = time.strftime(\"%d%m%Y_%H%M%S\")\n",
    "\n",
    "cols = ['F1', 'F2', 'F3', 'F4', 'F5', 'F6', 'F7', 'F8', 'F9', 'F10',\n",
    "       'F11', 'F12', 'F13', 'F14', 'F15', 'F16', 'F17', 'F18', 'F19', 'F20',\n",
    "       'F21', 'F22', 'F23', 'F24', 'F25', 'F26', 'F27', 'F28']\n",
    "\n",
    "submission[cols] = submission[cols].mask(submission[cols] < 0, 0)\n",
    "result.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
